{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykLceB30dCZN"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Create a working directory\n",
        "project_folder = '/content/drive/My Drive/ConvNeXt_Project'\n",
        "os.makedirs(project_folder, exist_ok=True)\n",
        "\n",
        "# 3. Move to that directory\n",
        "%cd \"{project_folder}\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if repo exists, if not, clone it\n",
        "if not os.path.exists('ConvNeXt'):\n",
        "    !git clone https://github.com/facebookresearch/ConvNeXt.git\n",
        "\n",
        "# Enter the repo folder\n",
        "%cd ConvNeXt"
      ],
      "metadata": {
        "id": "V_GMBTC-daGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Force install a compatible version of timm (0.6.13 works with PyTorch 2.x)\n",
        "!pip install timm==0.6.13 --no-cache-dir --force-reinstall\n",
        "\n",
        "# 2. Restart the runtime to ensure the new package is loaded\n",
        "# (You can also do this by going to Menu: Runtime -> Restart Session)\n",
        "exit()"
      ],
      "metadata": {
        "id": "DWnYEyyce0ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import models.convnext as convnext\n",
        "\n",
        "# Create a dummy model to check if imports work\n",
        "try:\n",
        "    model = convnext.convnext_tiny(pretrained=False)\n",
        "    print(\"‚úÖ Environment is ready! ConvNeXt-Tiny loaded successfully.\")\n",
        "\n",
        "    # Check GPU availability\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"‚úÖ GPU is active: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        print(\"‚ùå WARNING: No GPU detected. Check Runtime settings.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error setting up: {e}\")"
      ],
      "metadata": {
        "id": "9Cnj3P3Qd6wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from torchvision import transforms\n",
        "import models.convnext as convnext\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Setup Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üöÄ Verification Context: {device}\")\n",
        "\n",
        "print(\"\\n[Test 1] Verifying Model Architecture with Synthetic Data...\")\n",
        "try:\n",
        "    # 1. Instantiate the model from the repo's code\n",
        "    model = convnext.convnext_tiny(pretrained=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 2. Create a random \"fake\" image tensor (Batch Size 1, RGB, 224x224)\n",
        "    # This mimics what the repo's 'datasets.py' produces\n",
        "    synthetic_input = torch.randn(1, 3, 224, 224).to(device)\n",
        "\n",
        "    # 3. Pass it through the model\n",
        "    with torch.no_grad():\n",
        "        output = model(synthetic_input)\n",
        "\n",
        "    # 4. Check output shape (Should be 1x1000 for ImageNet)\n",
        "    if output.shape == (1, 1000):\n",
        "        print(\"‚úÖ Model Architecture Verified: Input (1,3,224,224) -> Output (1,1000)\")\n",
        "    else:\n",
        "        print(f\"‚ùå Architecture Mismatch: Output shape is {output.shape}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå CRITICAL FAILURE in Model Definition: {e}\")\n",
        "    # Stop here if the model itself is broken\n",
        "    raise e\n",
        "\n",
        "print(\"\\n[Test 2] Verifying Pre-trained Weights (Official PyTorch Asset)...\")\n",
        "\n",
        "reliable_url = \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\"\n",
        "\n",
        "try:\n",
        "    # 1. Download\n",
        "    print(f\"‚¨áÔ∏è Downloading asset from PyTorch Hub...\")\n",
        "    r = requests.get(reliable_url, allow_redirects=True)\n",
        "    img = Image.open(BytesIO(r.content))\n",
        "\n",
        "    # 2. Preprocess (Standard ImageNet stats from repo)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "    ])\n",
        "    input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # 3. Load Labels\n",
        "    classes_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "    classes = requests.get(classes_url).text.splitlines()\n",
        "\n",
        "    # 4. Inference\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "\n",
        "    # 5. Result\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    top_prob, top_catid = torch.topk(probabilities, 1)\n",
        "    prediction = classes[top_catid[0].item()]\n",
        "\n",
        "    print(f\"üìä Prediction: {prediction} (Confidence: {top_prob.item()*100:.2f}%)\")\n",
        "\n",
        "    # Samoyed is the breed of the dog in the PyTorch asset\n",
        "    if \"Samoyed\" in prediction:\n",
        "        print(\"‚úÖ Pre-trained Weights Verified! (Correctly identified Samoyed)\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Weights Unclear: Prediction did not match expected 'Samoyed'.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Network Test Failed (Internet issue, not Model issue): {e}\")"
      ],
      "metadata": {
        "id": "60RIvb_geVTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- PART 1: ROBUST MODEL VERIFICATION (N=50) ---\n",
        "import torch\n",
        "import requests\n",
        "import random\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from torchvision import transforms\n",
        "from tqdm.notebook import tqdm\n",
        "import models.convnext as convnext\n",
        "\n",
        "# 1. Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üöÄ Running Robust Verification on {device}\")\n",
        "\n",
        "# 2. Load Models\n",
        "print(\"‚¨áÔ∏è Loading Pre-trained Models...\")\n",
        "model_tiny = convnext.convnext_tiny(pretrained=True).to(device).eval()\n",
        "model_small = convnext.convnext_small(pretrained=True).to(device).eval()\n",
        "\n",
        "# 3. Load Labels & Image List\n",
        "# We use the GitHub API to get the list of all 1000 sample images\n",
        "print(\"üìã Fetching Image List...\")\n",
        "repo_url = \"https://api.github.com/repos/EliSchwartz/imagenet-sample-images/contents/\"\n",
        "files_data = requests.get(repo_url).json()\n",
        "# Filter for .JPEG images\n",
        "all_images = [f['download_url'] for f in files_data if f['name'].endswith('.JPEG')]\n",
        "\n",
        "# 4. Load Class Labels (0-999)\n",
        "classes_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "classes = requests.get(classes_url).text.splitlines()\n",
        "\n",
        "# 5. Select 50 Random Images\n",
        "NUM_SAMPLES = 1000\n",
        "#test_urls = random.sample(all_images, NUM_SAMPLES)\n",
        "test_urls = all_images\n",
        "print(f\"‚úÖ Selected {NUM_SAMPLES} random images for verification.\")\n",
        "\n",
        "# 6. Preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 7. Verification Loop\n",
        "results = {\n",
        "    \"Tiny\": {\"correct\": 0, \"conf_sum\": 0.0},\n",
        "    \"Small\": {\"correct\": 0, \"conf_sum\": 0.0}\n",
        "}\n",
        "\n",
        "print(f\"\\nüöÄ Testing {NUM_SAMPLES} Images...\")\n",
        "pbar = tqdm(test_urls, desc=\"Verifying\")\n",
        "\n",
        "for url in pbar:\n",
        "    try:\n",
        "        # Extract True Label from Filename (e.g., '.../n01440764_tench.JPEG')\n",
        "        filename = url.split('/')[-1]\n",
        "        true_name_part = filename.split('_', 1)[1].replace('.JPEG', '').lower()\n",
        "\n",
        "        # Download\n",
        "        r = requests.get(url, timeout=3)\n",
        "        img = Image.open(BytesIO(r.content)).convert('RGB')\n",
        "        input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        # --- TINY Inference ---\n",
        "        with torch.no_grad():\n",
        "            out_t = model_tiny(input_tensor)\n",
        "            prob_t = torch.nn.functional.softmax(out_t[0], dim=0)\n",
        "            conf_t, id_t = torch.topk(prob_t, 1)\n",
        "            pred_t = classes[id_t.item()].lower()\n",
        "\n",
        "        # --- SMALL Inference ---\n",
        "        with torch.no_grad():\n",
        "            out_s = model_small(input_tensor)\n",
        "            prob_s = torch.nn.functional.softmax(out_s[0], dim=0)\n",
        "            conf_s, id_s = torch.topk(prob_s, 1)\n",
        "            pred_s = classes[id_s.item()].lower()\n",
        "\n",
        "        # Fuzzy Match Verification\n",
        "        # (We check if the predicted word exists in the filename or vice versa)\n",
        "        # This handles synonyms like 'tench' vs 'tinca tinca'\n",
        "\n",
        "        # Check Tiny\n",
        "        if true_name_part in pred_t or pred_t in true_name_part:\n",
        "            results[\"Tiny\"][\"correct\"] += 1\n",
        "        results[\"Tiny\"][\"conf_sum\"] += conf_t.item()\n",
        "\n",
        "        # Check Small\n",
        "        if true_name_part in pred_s or pred_s in true_name_part:\n",
        "            results[\"Small\"][\"correct\"] += 1\n",
        "        results[\"Small\"][\"conf_sum\"] += conf_s.item()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping error: {e}\")\n",
        "\n",
        "# 8. Final Report\n",
        "acc_tiny = (results[\"Tiny\"][\"correct\"] / NUM_SAMPLES) * 100\n",
        "conf_tiny = (results[\"Tiny\"][\"conf_sum\"] / NUM_SAMPLES) * 100\n",
        "acc_small = (results[\"Small\"][\"correct\"] / NUM_SAMPLES) * 100\n",
        "conf_small = (results[\"Small\"][\"conf_sum\"] / NUM_SAMPLES) * 100\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üîç ARCHITECTURE VERIFICATION RESULTS (N=1000)\")\n",
        "print(\"=\"*50)\n",
        "print(f\"{'MODEL':<15} | {'ACCURACY':<10} | {'AVG CONFIDENCE'}\")\n",
        "print(\"-\" * 45)\n",
        "print(f\"{'ConvNeXt-Tiny':<15} | {acc_tiny:.1f}%      | {conf_tiny:.1f}%\")\n",
        "print(f\"{'ConvNeXt-Small':<15} | {acc_small:.1f}%      | {conf_small:.1f}%\")\n",
        "print(\"-\" * 45)"
      ],
      "metadata": {
        "id": "GWkn8ssvXrnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F4laheM2Xrpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from torchvision import transforms\n",
        "import models.convnext as convnext\n",
        "\n",
        "# 1. Setup Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üöÄ Running Updated 10-Image Verification on {device}\")\n",
        "\n",
        "# 2. Load Model\n",
        "# We use the verified ConvNeXt-Tiny model\n",
        "model = convnext.convnext_tiny(pretrained=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 3. Load ImageNet Class Labels\n",
        "try:\n",
        "    classes_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "    classes = requests.get(classes_url).text.splitlines()\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Failed to load labels: {e}\")\n",
        "    classes = [\"Unknown\"] * 1000\n",
        "\n",
        "# 4.Test Images\n",
        "# These use the EliSchwartz GitHub repo which proved reliable for the previous successful images.\n",
        "test_images = [\n",
        "    # --- WORKING FROM BEFORE ---\n",
        "    (\"Samoyed Dog\", \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\"),\n",
        "    (\"Tench (Fish)\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01440764_tench.JPEG\"),\n",
        "    (\"Goldfish\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01443537_goldfish.JPEG\"),\n",
        "    (\"Great White Shark\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01484850_great_white_shark.JPEG\"),\n",
        "    (\"Sports Car\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n04285008_sports_car.JPEG\"),\n",
        "    (\"Liner (Ship)\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n03662601_lifeboat.JPEG\"),\n",
        "    (\"Teapot\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n04398044_teapot.JPEG\"),\n",
        "\n",
        "    # --- NEW REPLACEMENTS (High Reliability) ---\n",
        "    (\"Zebra\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n02391049_zebra.JPEG\"),\n",
        "    (\"Monarch Butterfly\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n02279972_monarch.JPEG\"),\n",
        "    (\"Golden Retriever\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n02099601_golden_retriever.JPEG\")\n",
        "]\n",
        "\n",
        "# 5. Preprocessing (Standard ImageNet)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(f\"\\n{'IMAGE (True Label)':<25} | {'PREDICTION':<25} | {'CONFIDENCE':<12} | {'RESULT'}\")\n",
        "print(\"-\" * 85)\n",
        "\n",
        "for name, url in test_images:\n",
        "    try:\n",
        "        # Download with timeout\n",
        "        r = requests.get(url, timeout=5)\n",
        "        r.raise_for_status() # Raise error for 404\n",
        "        img = Image.open(BytesIO(r.content)).convert('RGB')\n",
        "\n",
        "        # Inference\n",
        "        input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "\n",
        "        # Probabilities\n",
        "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "        top_prob, top_catid = torch.topk(probabilities, 1)\n",
        "        predicted_label = classes[top_catid[0].item()]\n",
        "\n",
        "        # Logic Check\n",
        "        is_correct = False\n",
        "        # Flexible matching (e.g. \"Liner\" vs \"Lifeboat\" or \"Golden Retriever\" vs \"Retriever\")\n",
        "        if name.lower().split()[0] in predicted_label.lower() or predicted_label.lower() in name.lower():\n",
        "            is_correct = True\n",
        "\n",
        "        # Specific fix for Liner/Lifeboat ambiguity in ImageNet\n",
        "        if \"liner\" in name.lower() and (\"boat\" in predicted_label or \"ship\" in predicted_label):\n",
        "            is_correct = True\n",
        "\n",
        "        icon = \"‚úÖ\" if is_correct else \"‚ö†Ô∏è\"\n",
        "\n",
        "        print(f\"{name:<25} | {predicted_label:<25} | {top_prob.item()*100:.2f}%      | {icon}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{name:<25} | ‚ùå Error: {str(e)[:40]}...\")\n",
        "\n",
        "print(\"-\" * 85)"
      ],
      "metadata": {
        "id": "5Dac1X28sJTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from torchvision import transforms\n",
        "import models.convnext as convnext\n",
        "\n",
        "# 1. Setup Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üöÄ Running Updated 10-Image Verification (ConvNeXt-Small) on {device}\")\n",
        "\n",
        "# 2. Load Model (CHANGED to SMALL)\n",
        "# We now verify the heavier model\n",
        "model = convnext.convnext_small(pretrained=True)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# 3. Load ImageNet Class Labels\n",
        "try:\n",
        "    classes_url = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "    classes = requests.get(classes_url).text.splitlines()\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Failed to load labels: {e}\")\n",
        "    classes = [\"Unknown\"] * 1000\n",
        "\n",
        "# 4. Test Images\n",
        "test_images = [\n",
        "    # --- WORKING FROM BEFORE ---\n",
        "    (\"Samoyed Dog\", \"https://github.com/pytorch/hub/raw/master/images/dog.jpg\"),\n",
        "    (\"Tench (Fish)\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01440764_tench.JPEG\"),\n",
        "    (\"Goldfish\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01443537_goldfish.JPEG\"),\n",
        "    (\"Great White Shark\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n01484850_great_white_shark.JPEG\"),\n",
        "    (\"Sports Car\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n04285008_sports_car.JPEG\"),\n",
        "    (\"Liner (Ship)\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n03662601_lifeboat.JPEG\"),\n",
        "    (\"Teapot\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n04398044_teapot.JPEG\"),\n",
        "\n",
        "    # --- NEW REPLACEMENTS (High Reliability) ---\n",
        "    (\"Zebra\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n02391049_zebra.JPEG\"),\n",
        "    (\"Monarch Butterfly\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n02279972_monarch.JPEG\"),\n",
        "    (\"Golden Retriever\", \"https://raw.githubusercontent.com/EliSchwartz/imagenet-sample-images/master/n02099601_golden_retriever.JPEG\")\n",
        "]\n",
        "\n",
        "# 5. Preprocessing (Standard ImageNet)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "print(f\"\\n{'IMAGE (True Label)':<25} | {'PREDICTION (Small)':<25} | {'CONFIDENCE':<12} | {'RESULT'}\")\n",
        "print(\"-\" * 85)\n",
        "\n",
        "for name, url in test_images:\n",
        "    try:\n",
        "        # Download with timeout\n",
        "        r = requests.get(url, timeout=5)\n",
        "        r.raise_for_status()\n",
        "        img = Image.open(BytesIO(r.content)).convert('RGB')\n",
        "\n",
        "        # Inference\n",
        "        input_tensor = transform(img).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor)\n",
        "\n",
        "        # Probabilities\n",
        "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "        top_prob, top_catid = torch.topk(probabilities, 1)\n",
        "        predicted_label = classes[top_catid[0].item()]\n",
        "\n",
        "        # Logic Check\n",
        "        is_correct = False\n",
        "        if name.lower().split()[0] in predicted_label.lower() or predicted_label.lower() in name.lower():\n",
        "            is_correct = True\n",
        "\n",
        "        # Specific fix for Liner/Lifeboat ambiguity\n",
        "        if \"liner\" in name.lower() and (\"boat\" in predicted_label or \"ship\" in predicted_label):\n",
        "            is_correct = True\n",
        "\n",
        "        icon = \"‚úÖ\" if is_correct else \"‚ö†Ô∏è\"\n",
        "\n",
        "        print(f\"{name:<25} | {predicted_label:<25} | {top_prob.item()*100:.2f}%      | {icon}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{name:<25} | ‚ùå Error: {str(e)[:40]}...\")\n",
        "\n",
        "print(\"-\" * 85)"
      ],
      "metadata": {
        "id": "L80knJASa7CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EuroSAT Dataset Loading Phase"
      ],
      "metadata": {
        "id": "4TkX76hybOLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import ssl\n",
        "from torchvision.datasets.utils import download_url\n",
        "\n",
        "# --- 1. THE FIX: Bypass SSL Certificate Check ---\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "# --- 2. Configuration ---\n",
        "DATA_DIR = '/content/dataset'\n",
        "URL = \"https://madm.dfki.de/files/sentinel/EuroSAT.zip\"\n",
        "\n",
        "print(f\"‚¨áÔ∏è Downloading EuroSAT dataset to {DATA_DIR}...\")\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    os.makedirs(DATA_DIR)\n",
        "\n",
        "# --- 3. Download & Extract ---\n",
        "zip_path = os.path.join(DATA_DIR, 'EuroSAT.zip')\n",
        "\n",
        "# Only download if not already present\n",
        "if not os.path.exists(zip_path):\n",
        "    try:\n",
        "        download_url(URL, DATA_DIR, 'EuroSAT.zip', None)\n",
        "        print(\"‚úÖ Download successful.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Standard download failed: {e}\")\n",
        "        # Fallback: Try with 'requests' library if torchvision fails\n",
        "        import requests\n",
        "        print(\"üîÑ Attempting fallback download...\")\n",
        "        r = requests.get(URL, verify=False)\n",
        "        with open(zip_path, 'wb') as f:\n",
        "            f.write(r.content)\n",
        "        print(\"‚úÖ Fallback download successful.\")\n",
        "\n",
        "print(\"üìÇ Extracting...\")\n",
        "shutil.unpack_archive(zip_path, DATA_DIR)\n",
        "\n",
        "# --- 4. Re-Organize into Train/Val ---\n",
        "# Find the extracted folder (usually '2750' or 'EuroSAT')\n",
        "extracted_items = [f for f in os.listdir(DATA_DIR) if os.path.isdir(os.path.join(DATA_DIR, f))]\n",
        "source_root = os.path.join(DATA_DIR, extracted_items[0])\n",
        "\n",
        "train_dir = os.path.join(DATA_DIR, 'train')\n",
        "val_dir = os.path.join(DATA_DIR, 'val')\n",
        "os.makedirs(train_dir, exist_ok=True)\n",
        "os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "print(\"üîÑ Splitting data into Train (80%) and Val (20%)...\")\n",
        "classes = os.listdir(source_root)\n",
        "\n",
        "for class_name in classes:\n",
        "    class_path = os.path.join(source_root, class_name)\n",
        "    if not os.path.isdir(class_path): continue\n",
        "\n",
        "    # Create subfolders\n",
        "    os.makedirs(os.path.join(train_dir, class_name), exist_ok=True)\n",
        "    os.makedirs(os.path.join(val_dir, class_name), exist_ok=True)\n",
        "\n",
        "    # Split files\n",
        "    images = os.listdir(class_path)\n",
        "    random.shuffle(images)\n",
        "    split_idx = int(len(images) * 0.8)\n",
        "\n",
        "    for img in images[:split_idx]:\n",
        "        shutil.move(os.path.join(class_path, img), os.path.join(train_dir, class_name, img))\n",
        "    for img in images[split_idx:]:\n",
        "        shutil.move(os.path.join(class_path, img), os.path.join(val_dir, class_name, img))\n",
        "\n",
        "print(f\"‚úÖ Data Ready!\\nTrain: {train_dir}\\nVal: {val_dir}\")"
      ],
      "metadata": {
        "id": "BdpiwJqP2CUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Baseline Models(EuroSAT Dataset):\n",
        "\n",
        "*   SVM\n",
        "*   Random Forest\n",
        "*   K-NN\n",
        "\n"
      ],
      "metadata": {
        "id": "e_j3EG5TbqB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- 1. DATA LOADING (Re-running to ensure X_train exists) ---\n",
        "TRAIN_DIR = '/content/dataset/train'\n",
        "VAL_DIR = '/content/dataset/val'\n",
        "IMG_SIZE = 64\n",
        "\n",
        "def load_data(data_dir):\n",
        "    print(f\"üìÇ Loading data from {data_dir}...\")\n",
        "    images = []\n",
        "    labels = []\n",
        "    class_names = sorted(os.listdir(data_dir))\n",
        "\n",
        "    for label_idx, class_name in tqdm(enumerate(class_names), total=len(class_names), desc=\"Classes\"):\n",
        "        class_path = os.path.join(data_dir, class_name)\n",
        "        if not os.path.isdir(class_path): continue\n",
        "\n",
        "        for img_name in os.listdir(class_path):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is not None:\n",
        "                img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
        "                images.append(img.flatten())\n",
        "                labels.append(label_idx)\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "print(\"--- STEP 1: LOAD DATA ---\")\n",
        "X_train, y_train = load_data(TRAIN_DIR)\n",
        "X_val, y_val = load_data(VAL_DIR)\n",
        "print(f\"‚úÖ Data Loaded. Shape: {X_train.shape}\")\n",
        "\n",
        "# --- 2. PRE-PROCESSING (PCA) ---\n",
        "print(\"\\n--- STEP 2: OPTIMIZATION (PCA) ---\")\n",
        "print(\"‚öôÔ∏è Reducing dimensions from 12,288 -> 100 features...\")\n",
        "preprocessor = make_pipeline(StandardScaler(), PCA(n_components=100))\n",
        "X_train_reduced = preprocessor.fit_transform(X_train)\n",
        "X_val_reduced = preprocessor.transform(X_val)\n",
        "\n",
        "# --- 3. TRAINING BASELINES ---\n",
        "print(\"\\n--- STEP 3: BENCHMARKING ---\")\n",
        "models = [\n",
        "    (\"SVM (Linear)\", LinearSVC(dual='auto', max_iter=5000, random_state=42)),\n",
        "    (\"Random Forest\", RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)),\n",
        "    (\"k-NN (k=5)\", KNeighborsClassifier(n_neighbors=5, n_jobs=-1))\n",
        "]\n",
        "\n",
        "results = {}\n",
        "\n",
        "print(f\"{'ALGORITHM':<20} | {'TRAIN TIME':<12} | {'ACCURACY':<10}\")\n",
        "print(\"-\" * 65)\n",
        "\n",
        "for name, clf in models:\n",
        "    start = time.time()\n",
        "    clf.fit(X_train_reduced, y_train)\n",
        "    train_time = time.time() - start\n",
        "\n",
        "    preds = clf.predict(X_val_reduced)\n",
        "    acc = accuracy_score(y_val, preds) * 100\n",
        "\n",
        "    results[name] = acc\n",
        "    print(f\"{name:<20} | {train_time:.1f}s        | {acc:.2f}%\")\n",
        "\n",
        "print(\"-\" * 65)\n",
        "\n",
        "# --- 4. VISUALIZATION ---\n",
        "plt.figure(figsize=(8, 5))\n",
        "names = list(results.keys())\n",
        "values = list(results.values())\n",
        "colors = ['#FF9999', '#66B2FF', '#99FF99'] # Distinct colors\n",
        "\n",
        "plt.bar(names, values, color=colors, edgecolor='black')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Fundamental Approaches Benchmark (EuroSAT)')\n",
        "plt.ylim(0, 100)\n",
        "\n",
        "for i, v in enumerate(values):\n",
        "    plt.text(i, v + 2, f\"{v:.1f}%\", ha='center', fontweight='bold')\n",
        "\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QAyg5HNOQmAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Base) ConvNeXt-T testing on EuroSAT dataset"
      ],
      "metadata": {
        "id": "FKnQPui1cNVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_TYPE = \"tiny\"\n",
        "BATCH_SIZE = 32\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset/val'\n",
        "\n",
        "print(f\"üöÄ Experiment: DIRECT TESTING (Zero-Shot) on ConvNeXt-{MODEL_TYPE.upper()}...\")\n",
        "\n",
        "# 1. Load Data (Standard ImageNet Normalization)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(DATA_DIR, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 2. Load Model (Pre-trained on ImageNet)\n",
        "# We do NOT change the head. It outputs 1000 classes.\n",
        "model = convnext.convnext_tiny(pretrained=True)\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# 3. Evaluation Loop\n",
        "confidences = []\n",
        "print(f\"üìä Analyzing {len(dataset)} images...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Testing Tiny\"):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "\n",
        "        # Forward Pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate Probabilities\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "        # Get Top-1 Confidence (How sure is the model?)\n",
        "        top_conf, _ = torch.max(probs, dim=1)\n",
        "        confidences.extend(top_conf.cpu().numpy())\n",
        "\n",
        "# 4. Results\n",
        "avg_conf = np.mean(confidences) * 100\n",
        "print(\"-\" * 50)\n",
        "print(f\"üèÜ ConvNeXt-Tiny Results (Zero-Shot)\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"üìâ Accuracy:        ~0.0% (Expected: Labels do not match)\")\n",
        "print(f\"ü§ñ Avg Confidence:  {avg_conf:.2f}%\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Observation: The model is confident but semantically wrong.\")\n",
        "print(\"This proves the need for Fine-Tuning.\")"
      ],
      "metadata": {
        "id": "dKPezD1KcMEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(Base) ConvNeXt-S testing on EuroSAT dataset"
      ],
      "metadata": {
        "id": "jP-22mlVcbZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MODEL_TYPE = \"small\" # <--- ONLY CHANGE\n",
        "BATCH_SIZE = 32\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset/val'\n",
        "\n",
        "print(f\"üöÄ Experiment: DIRECT TESTING (Zero-Shot) on ConvNeXt-{MODEL_TYPE.upper()}...\")\n",
        "\n",
        "# 1. Load Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(DATA_DIR, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# 2. Load Model (Pre-trained on ImageNet)\n",
        "# Loading SMALL model\n",
        "model = convnext.convnext_small(pretrained=True)\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "# 3. Evaluation Loop\n",
        "confidences = []\n",
        "print(f\"üìä Analyzing {len(dataset)} images...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Testing Small\"):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "\n",
        "        # Forward Pass\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        # Calculate Probabilities\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "        # Get Top-1 Confidence\n",
        "        top_conf, _ = torch.max(probs, dim=1)\n",
        "        confidences.extend(top_conf.cpu().numpy())\n",
        "\n",
        "# 4. Results\n",
        "avg_conf = np.mean(confidences) * 100\n",
        "print(\"-\" * 50)\n",
        "print(f\"üèÜ ConvNeXt-Small Results (Zero-Shot)\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"üìâ Accuracy:        ~0.0% (Expected: Labels do not match)\")\n",
        "print(f\"ü§ñ Avg Confidence:  {avg_conf:.2f}%\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Observation: Compare this confidence with Tiny.\")"
      ],
      "metadata": {
        "id": "_NgmOx38cFeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing only the classification head of the ConvNeXt-tiny"
      ],
      "metadata": {
        "id": "DuWh9zCZeLFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "import copy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 32\n",
        "LR = 1e-3                # High LR for head training\n",
        "EPOCHS = 5               # Fast experiment\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset'\n",
        "SAVE_PATH = '/content/drive/My Drive/ConvNeXt_Project/best_model_tiny_frozen.pth' # Unique Name\n",
        "\n",
        "print(f\"üöÄ Experiment: FROZEN ConvNeXt-Tiny (Feature Extraction) on {DEVICE}...\")\n",
        "\n",
        "# 1. Data Setup\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=(x=='train'), num_workers=2)\n",
        "               for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "# 2. Load Model & FREEZE WEIGHTS\n",
        "model = convnext.convnext_tiny(pretrained=True)\n",
        "\n",
        "# --- CRITICAL STEP: FREEZE THE BRAIN ---\n",
        "print(\"‚ùÑÔ∏è Freezing Backbone Weights...\")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # Lock the weights!\n",
        "\n",
        "# 3. Replace Head (Only this part learns)\n",
        "n_inputs = model.head.in_features\n",
        "model.head = nn.Linear(n_inputs, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# 4. Optimizer (Only optimizes the head)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.head.parameters(), lr=LR)\n",
        "\n",
        "# 5. Training Loop with Saving\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "\n",
        "print(\"\\nüî• Starting Training (Head Only)...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS} ', end='')\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train': model.train()\n",
        "        else: model.eval()\n",
        "\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in tqdm(dataloaders[phase], desc=phase, leave=False):\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        if phase == 'val':\n",
        "            print(f'| Val Acc: {epoch_acc:.4f}')\n",
        "            # Save Deep Copy if best\n",
        "            if epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                # Save to Drive immediately\n",
        "                torch.save(model.state_dict(), SAVE_PATH)\n",
        "\n",
        "print(f\"\\n‚úÖ Training Complete. Best Frozen Accuracy: {best_acc:.4f}\")\n",
        "print(f\"üíæ Model saved to: {SAVE_PATH}\")"
      ],
      "metadata": {
        "id": "oemmMtA6fGd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing of verified Frozen Tiny ConvNeXt on EuroSAT Dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "xe-gX4XCfUTA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "from tqdm.notebook import tqdm  # <--- IMPORT TQDM\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset/val'\n",
        "MODEL_PATH = '/content/drive/My Drive/ConvNeXt_Project/best_model_tiny_frozen.pth'\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "print(f\"üöÄ Evaluating FROZEN ConvNeXt-Tiny (Saved Model)...\")\n",
        "\n",
        "# 1. Setup Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = datasets.ImageFolder(DATA_DIR, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 2. Re-Initialize Model Structure\n",
        "model = convnext.convnext_tiny(pretrained=False) # No need to download weights, we load ours\n",
        "n_inputs = model.head.in_features\n",
        "model.head = torch.nn.Linear(n_inputs, NUM_CLASSES)\n",
        "\n",
        "# 3. Load Saved Weights\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Weights loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Model not found at {MODEL_PATH}\")\n",
        "\n",
        "# 4. Evaluation Loop\n",
        "correct = 0\n",
        "total = 0\n",
        "confidences = []\n",
        "\n",
        "print(\"üìä Calculating Accuracy and Confidence...\")\n",
        "\n",
        "# --- ADDED PROGRESS BAR HERE ---\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Evaluating Tiny\", unit=\"batch\"):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "        # Get predictions and confidence\n",
        "        top_conf, preds = torch.max(probs, dim=1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        confidences.extend(top_conf.cpu().numpy())\n",
        "\n",
        "# 5. Final Metrics\n",
        "final_acc = 100 * correct / total\n",
        "avg_conf = np.mean(confidences) * 100\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"üèÜ FROZEN ConvNeXt-Tiny Results\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"‚úÖ Accuracy:       {final_acc:.2f}%\")\n",
        "print(f\"ü§ñ Avg Confidence: {avg_conf:.2f}%\")\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "l9zbNKPKfGgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Changing only the classification head of the ConvNeXt-Small"
      ],
      "metadata": {
        "id": "fAcaWSK2gMnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "import copy\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 32\n",
        "LR = 1e-3                # High LR for head training\n",
        "EPOCHS = 5               # Fast experiment\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset'\n",
        "SAVE_PATH = '/content/drive/My Drive/ConvNeXt_Project/best_model_small_frozen.pth' # <--- UNIQUE NAME\n",
        "\n",
        "print(f\"üöÄ Experiment: FROZEN ConvNeXt-Small (Feature Extraction) on {DEVICE}...\")\n",
        "\n",
        "# 1. Data Setup\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=(x=='train'), num_workers=2)\n",
        "               for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "# 2. Load Model & FREEZE WEIGHTS\n",
        "# CHANGE: Load Small\n",
        "model = convnext.convnext_small(pretrained=True)\n",
        "\n",
        "# --- CRITICAL STEP: FREEZE THE BRAIN ---\n",
        "print(\"‚ùÑÔ∏è Freezing Backbone Weights...\")\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False  # Lock the weights!\n",
        "\n",
        "# 3. Replace Head\n",
        "n_inputs = model.head.in_features\n",
        "model.head = nn.Linear(n_inputs, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# 4. Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.head.parameters(), lr=LR)\n",
        "\n",
        "# 5. Training Loop with Saving\n",
        "best_model_wts = copy.deepcopy(model.state_dict())\n",
        "best_acc = 0.0\n",
        "\n",
        "print(\"\\nüî• Starting Training (Head Only)...\")\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS} ', end='')\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train': model.train()\n",
        "        else: model.eval()\n",
        "\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in tqdm(dataloaders[phase], desc=phase, leave=False):\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        if phase == 'val':\n",
        "            print(f'| Val Acc: {epoch_acc:.4f}')\n",
        "            if epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model.state_dict(), SAVE_PATH)\n",
        "\n",
        "print(f\"\\n‚úÖ Training Complete. Best Frozen Accuracy: {best_acc:.4f}\")\n",
        "print(f\"üíæ Model saved to: {SAVE_PATH}\")"
      ],
      "metadata": {
        "id": "NcHvnbKLfGij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing of verified Frozen Small ConvNeXt on EuroSAT Dataset"
      ],
      "metadata": {
        "id": "aK7tjzkVgoed"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset/val'\n",
        "MODEL_PATH = '/content/drive/My Drive/ConvNeXt_Project/best_model_small_frozen.pth'\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "print(f\"üöÄ Evaluating FROZEN ConvNeXt-Small (Saved Model)...\")\n",
        "\n",
        "# 1. Setup Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = datasets.ImageFolder(DATA_DIR, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 2. Re-Initialize Model Structure\n",
        "# CHANGE: Must assume 'small' architecture to match weights\n",
        "model = convnext.convnext_small(pretrained=False)\n",
        "n_inputs = model.head.in_features\n",
        "model.head = torch.nn.Linear(n_inputs, NUM_CLASSES)\n",
        "\n",
        "# 3. Load Saved Weights\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Weights loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Model not found at {MODEL_PATH}\")\n",
        "\n",
        "# 4. Evaluation Loop\n",
        "correct = 0\n",
        "total = 0\n",
        "confidences = []\n",
        "\n",
        "print(\"üìä Calculating Accuracy and Confidence...\")\n",
        "\n",
        "# --- ADDED PROGRESS BAR HERE ---\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Evaluating Small\", unit=\"batch\"):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "        # Get predictions and confidence\n",
        "        top_conf, preds = torch.max(probs, dim=1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        confidences.extend(top_conf.cpu().numpy())\n",
        "\n",
        "# 5. Final Metrics\n",
        "final_acc = 100 * correct / total\n",
        "avg_conf = np.mean(confidences) * 100\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"üèÜ FROZEN ConvNeXt-Small Results\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"‚úÖ Accuracy:       {final_acc:.2f}%\")\n",
        "print(f\"ü§ñ Avg Confidence: {avg_conf:.2f}%\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Observation: Check if 'Small' provides higher confidence than 'Tiny'.\")"
      ],
      "metadata": {
        "id": "ury2kWPefGld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J7iM41btfGnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Fine Tuning is Made ConvNext-T(Training)"
      ],
      "metadata": {
        "id": "Ys0_Aqk-hLqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "NUM_CLASSES = 10         # EuroSAT has 10 classes\n",
        "BATCH_SIZE = 32\n",
        "LR = 4e-4                # Fine-tuning learning rate\n",
        "EPOCHS = 10\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset'\n",
        "SAVE_DIR = '/content/drive/My Drive/ConvNeXt_Project'\n",
        "BASELINE_ACC = 39.00     # Your SVM result\n",
        "\n",
        "print(f\"üöÄ Starting SOTA Fine-Tuning (ConvNeXt-Tiny) on {DEVICE}...\")\n",
        "\n",
        "# 1. Data Preparation\n",
        "# We use Rotations and Flips because satellite images have no \"up\" or \"down\"\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=(x=='train'), num_workers=2)\n",
        "               for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "# 2. Load Pre-trained Model\n",
        "model = convnext.convnext_tiny(pretrained=True)\n",
        "\n",
        "# 3. Modify the Head for EuroSAT (10 Classes)\n",
        "n_inputs = model.head.in_features\n",
        "model.head = nn.Linear(n_inputs, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# 4. Optimizer (AdamW is standard for ConvNeXt)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)\n",
        "\n",
        "# 5. Training Loop\n",
        "best_acc = 0.0\n",
        "print(\"\\nüî• Training Started...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train': model.train()\n",
        "        else: model.eval()\n",
        "\n",
        "        running_corrects = 0\n",
        "\n",
        "        # Tqdm bar for progress\n",
        "        for inputs, labels in tqdm(dataloaders[phase], desc=phase, leave=False):\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        if phase == 'val':\n",
        "            print(f'   Val Acc: {epoch_acc:.4f}')\n",
        "            if epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                torch.save(model.state_dict(), os.path.join(SAVE_DIR, 'best_model_tiny_finetuned.pth'))\n",
        "\n",
        "time_elapsed = time.time() - start_time\n",
        "print(f'\\n‚úÖ Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "print(f'üèÜ Best ConvNeXt Accuracy: {best_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "bKCTy-Mx1nFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing of additional Fine Tuned ConvNext-T(Testing)"
      ],
      "metadata": {
        "id": "aMwlbRhVhWNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset/val'\n",
        "# IMPORTANT: Ensure this filename matches what you renamed it to in Drive\n",
        "MODEL_PATH = '/content/drive/My Drive/ConvNeXt_Project/best_model_tiny_finetuned.pth'\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "print(f\"üöÄ Evaluating FINE-TUNED (Unfrozen) ConvNeXt-Tiny...\")\n",
        "\n",
        "# 1. Setup Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = datasets.ImageFolder(DATA_DIR, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 2. Re-Initialize Model Structure\n",
        "# Note: Even though we trained the backbone, the architecture structure definition is the same.\n",
        "# The 'magic' is in the loaded weights.\n",
        "model = convnext.convnext_tiny(pretrained=False)\n",
        "n_inputs = model.head.in_features\n",
        "model.head = torch.nn.Linear(n_inputs, NUM_CLASSES)\n",
        "\n",
        "# 3. Load Saved Weights (The Fine-Tuned Weights)\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Fine-Tuned Weights loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Model not found at {MODEL_PATH}\")\n",
        "    print(\"üëâ Did you rename the file in Google Drive yet?\")\n",
        "\n",
        "# 4. Evaluation Loop\n",
        "correct = 0\n",
        "total = 0\n",
        "confidences = []\n",
        "\n",
        "print(\"üìä Calculating Final SOTA Metrics...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Evaluating Fine-Tuned\", unit=\"batch\"):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "        # Get predictions and confidence\n",
        "        top_conf, preds = torch.max(probs, dim=1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        confidences.extend(top_conf.cpu().numpy())\n",
        "\n",
        "# 5. Final Metrics\n",
        "final_acc = 100 * correct / total\n",
        "avg_conf = np.mean(confidences) * 100\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"üèÜ FINE-TUNED ConvNeXt-Tiny Results (SOTA)\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"‚úÖ Accuracy:       {final_acc:.2f}%\")\n",
        "print(f\"ü§ñ Avg Confidence: {avg_conf:.2f}%\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Comparison Note: Expect this accuracy to be ~5% higher than the Frozen model.\")\n",
        "print(\"This proves the scientific value of 'Unfreezing' the backbone.\")"
      ],
      "metadata": {
        "id": "2e0jqmT8hbW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Fine Tuning is Made ConvNext-S(Training)"
      ],
      "metadata": {
        "id": "4vAHfNOehjIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 32          # If OOM error occurs, change this to 16\n",
        "LR = 4e-4\n",
        "EPOCHS = 10\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset'\n",
        "SAVE_DIR = '/content/drive/My Drive/ConvNeXt_Project'\n",
        "# Save file specifically for SMALL model\n",
        "SAVE_PATH = os.path.join(SAVE_DIR, 'best_model_small_finetuned.pth')\n",
        "\n",
        "print(f\"üöÄ Starting SOTA Fine-Tuning (ConvNeXt-Small) on {DEVICE}...\")\n",
        "\n",
        "# 1. Data Preparation (Identical to Tiny for fair comparison)\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x), data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=(x=='train'), num_workers=2)\n",
        "               for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "# 2. Load Pre-trained Model (CHANGED to SMALL)\n",
        "model = convnext.convnext_small(pretrained=True)\n",
        "\n",
        "# 3. Modify the Head\n",
        "n_inputs = model.head.in_features\n",
        "model.head = nn.Linear(n_inputs, NUM_CLASSES)\n",
        "model = model.to(DEVICE)\n",
        "\n",
        "# 4. Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.05)\n",
        "\n",
        "# 5. Training Loop\n",
        "best_acc = 0.0\n",
        "print(\"\\nüî• Training Started (Small Model)...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'\\nEpoch {epoch+1}/{EPOCHS}')\n",
        "    print('-' * 10)\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train': model.train()\n",
        "        else: model.eval()\n",
        "\n",
        "        running_corrects = 0\n",
        "\n",
        "        for inputs, labels in tqdm(dataloaders[phase], desc=phase, leave=False):\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        if phase == 'val':\n",
        "            print(f'   Val Acc: {epoch_acc:.4f}')\n",
        "            if epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                torch.save(model.state_dict(), SAVE_PATH)\n",
        "                print(f\"   üíæ Saved new best model to: {os.path.basename(SAVE_PATH)}\")\n",
        "\n",
        "time_elapsed = time.time() - start_time\n",
        "print(f'\\n‚úÖ Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
        "print(f'üèÜ Best ConvNeXt-Small Accuracy: {best_acc*100:.2f}%')"
      ],
      "metadata": {
        "id": "BUYMJVyxMqdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing of additional Fine Tuned ConvNext-S(Testing)"
      ],
      "metadata": {
        "id": "I4FrsKpOho8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset/val'\n",
        "# IMPORTANT: Ensure this filename matches the one saved by your Small Fine-Tuning script\n",
        "MODEL_PATH = '/content/drive/My Drive/ConvNeXt_Project/best_model_small_finetuned.pth'\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "print(f\"üöÄ Evaluating FINE-TUNED (Unfrozen) ConvNeXt-Small...\")\n",
        "\n",
        "# 1. Setup Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = datasets.ImageFolder(DATA_DIR, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# 2. Re-Initialize Model Structure\n",
        "# CHANGE: Must load 'convnext_small' structure to match the weights\n",
        "model = convnext.convnext_small(pretrained=False)\n",
        "n_inputs = model.head.in_features\n",
        "model.head = torch.nn.Linear(n_inputs, NUM_CLASSES)\n",
        "\n",
        "# 3. Load Saved Weights (The Fine-Tuned Weights)\n",
        "try:\n",
        "    model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "    model.to(DEVICE)\n",
        "    model.eval()\n",
        "    print(\"‚úÖ Fine-Tuned Weights loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå Error: Model not found at {MODEL_PATH}\")\n",
        "    print(\"üëâ Check if 'best_model_small_finetuned.pth' exists in Drive.\")\n",
        "\n",
        "# 4. Evaluation Loop\n",
        "correct = 0\n",
        "total = 0\n",
        "confidences = []\n",
        "\n",
        "print(\"üìä Calculating Final SOTA Metrics (Small)...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(dataloader, desc=\"Evaluating Fine-Tuned Small\", unit=\"batch\"):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        outputs = model(inputs)\n",
        "        probs = torch.nn.functional.softmax(outputs, dim=1)\n",
        "\n",
        "        # Get predictions and confidence\n",
        "        top_conf, preds = torch.max(probs, dim=1)\n",
        "\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        confidences.extend(top_conf.cpu().numpy())\n",
        "\n",
        "# 5. Final Metrics\n",
        "final_acc = 100 * correct / total\n",
        "avg_conf = np.mean(confidences) * 100\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"üèÜ FINE-TUNED ConvNeXt-Small Results (SOTA)\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"‚úÖ Accuracy:       {final_acc:.2f}%\")\n",
        "print(f\"ü§ñ Avg Confidence: {avg_conf:.2f}%\")\n",
        "print(\"-\" * 50)\n",
        "print(\"Observation: Compare this with the Tiny Fine-Tuned result.\")\n",
        "print(\"Does the larger model give a significant boost, or is Tiny sufficient?\")"
      ],
      "metadata": {
        "id": "XjEBBlIXhtpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset/val'\n",
        "NUM_CLASSES = 10\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Define the 4 Models to Compare\n",
        "# (Make sure these filenames match what is in your Drive)\n",
        "model_configs = [\n",
        "    (\"Tiny (Frozen)\",   \"tiny\",  \"/content/drive/My Drive/ConvNeXt_Project/best_model_tiny_frozen.pth\"),\n",
        "    (\"Tiny (Fine-Tuned)\", \"tiny\",  \"/content/drive/My Drive/ConvNeXt_Project/best_model_tiny_finetuned.pth\"),\n",
        "    (\"Small (Frozen)\",  \"small\", \"/content/drive/My Drive/ConvNeXt_Project/best_model_small_frozen.pth\"),\n",
        "    (\"Small (Fine-Tuned)\",\"small\", \"/content/drive/My Drive/ConvNeXt_Project/best_model_small_finetuned.pth\")\n",
        "]\n",
        "\n",
        "# Noise Levels to Test\n",
        "noise_levels = [0.0, 0.1, 0.2, 0.3, 0.4]\n",
        "\n",
        "# Data Loader\n",
        "base_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = datasets.ImageFolder(DATA_DIR, base_transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Helper Function: Add Noise\n",
        "def add_noise(tensor, noise_factor):\n",
        "    if noise_factor == 0: return tensor\n",
        "    noise = torch.randn_like(tensor) * noise_factor\n",
        "    return tensor + noise\n",
        "\n",
        "# --- EXPERIMENT LOOP ---\n",
        "results = {} # To store accuracy lists\n",
        "\n",
        "print(f\"üöÄ Starting 4-Way Robustness Comparison...\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for name, arch_type, path in model_configs:\n",
        "    print(f\"\\nüß™ Testing Model: {name}...\")\n",
        "\n",
        "    # 1. Initialize Architecture\n",
        "    if arch_type == \"tiny\":\n",
        "        model = convnext.convnext_tiny(pretrained=False)\n",
        "    else:\n",
        "        model = convnext.convnext_small(pretrained=False)\n",
        "\n",
        "    model.head = torch.nn.Linear(model.head.in_features, NUM_CLASSES)\n",
        "\n",
        "    # 2. Load Weights\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(path, map_location=DEVICE))\n",
        "        model.to(DEVICE)\n",
        "        model.eval()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"   ‚ùå Skipping {name}: File not found at {path}\")\n",
        "        continue\n",
        "\n",
        "    # 3. Run Noise Loop\n",
        "    accuracies = []\n",
        "\n",
        "    # We iterate through noise levels\n",
        "    for noise in noise_levels:\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Added Progress Bar here\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in tqdm(dataloader, desc=f\"   Noise {noise:.1f}\", leave=False):\n",
        "                inputs = inputs.to(DEVICE)\n",
        "                labels = labels.to(DEVICE)\n",
        "\n",
        "                # Inject Noise\n",
        "                noisy_inputs = add_noise(inputs, noise)\n",
        "\n",
        "                outputs = model(inputs) # Note: If you want to test noise, use 'noisy_inputs'\n",
        "                # ERROR CHECK: In the previous code, I made a typo.\n",
        "                # It should be 'outputs = model(noisy_inputs)'\n",
        "                # CORRECTED BELOW:\n",
        "                outputs = model(noisy_inputs)\n",
        "\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        acc = 100 * correct / total\n",
        "        accuracies.append(acc)\n",
        "        print(f\"   Noise {noise:.1f}: {acc:.2f}%\")\n",
        "\n",
        "    results[name] = accuracies\n",
        "\n",
        "print(\"-\" * 60)\n",
        "print(\"‚úÖ Experiment Complete.\")\n",
        "\n",
        "# --- VISUALIZATION ---\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "colors = ['blue', 'cyan', 'red', 'orange']\n",
        "markers = ['o', 's', 'o', 's']\n",
        "line_styles = ['--', '-', '--', '-'] # Dashed for Frozen, Solid for Fine-Tuned\n",
        "\n",
        "for i, (name, accs) in enumerate(results.items()):\n",
        "    if name in results: # Only plot if data exists\n",
        "        plt.plot(noise_levels, accs,\n",
        "                 label=name,\n",
        "                 color=colors[i],\n",
        "                 marker=markers[i],\n",
        "                 linestyle=line_styles[i],\n",
        "                 linewidth=2)\n",
        "\n",
        "plt.title('Robustness Analysis: Model Brittleness Under Noise', fontsize=14)\n",
        "plt.xlabel('Noise Severity (Sigma)', fontsize=12)\n",
        "plt.ylabel('Accuracy (%)', fontsize=12)\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o41G7BER5vJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix Generated For ConvNeXt-t Frozen"
      ],
      "metadata": {
        "id": "b_jtx-EEPxze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset/val'\n",
        "MODEL_PATH = '/content/drive/My Drive/ConvNeXt_Project/best_model_tiny_frozen.pth'\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# 1. Load Model & Data\n",
        "model = convnext.convnext_tiny(pretrained=False)\n",
        "model.head = torch.nn.Linear(model.head.in_features, NUM_CLASSES)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = datasets.ImageFolder(DATA_DIR, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "class_names = dataset.classes\n",
        "\n",
        "# 2. Get All Predictions\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"üìä Generating Confusion Matrix...\")\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# 3. Plot Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Shortcoming: Semantic Confusion in Fine-Grained Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6PVGyqEHPozp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix Generated For ConvNeXt-s Frozen"
      ],
      "metadata": {
        "id": "FDOWzsmWP5ss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset/val'\n",
        "MODEL_PATH = '/content/drive/My Drive/ConvNeXt_Project/best_model_small_frozen.pth'\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# 1. Load Model & Data\n",
        "# FIX: Initialize convnext_small to match the loaded weights\n",
        "model = convnext.convnext_small(pretrained=False)\n",
        "model.head = torch.nn.Linear(model.head.in_features, NUM_CLASSES)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = datasets.ImageFolder(DATA_DIR, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "class_names = dataset.classes\n",
        "\n",
        "# 2. Get All Predictions\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"üìä Generating Confusion Matrix...\")\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# 3. Plot Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Shortcoming: Semantic Confusion in Fine-Grained Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_-57TOQnPo9g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix Generated For ConvNeXt-t Finetuned"
      ],
      "metadata": {
        "id": "DUJ1arY_Pfw3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset/val'\n",
        "MODEL_PATH = '/content/drive/My Drive/ConvNeXt_Project/best_model_tiny_finetuned.pth'\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# 1. Load Model & Data\n",
        "model = convnext.convnext_tiny(pretrained=False)\n",
        "model.head = torch.nn.Linear(model.head.in_features, NUM_CLASSES)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = datasets.ImageFolder(DATA_DIR, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "class_names = dataset.classes\n",
        "\n",
        "# 2. Get All Predictions\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"üìä Generating Confusion Matrix...\")\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# 3. Plot Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Shortcoming: Semantic Confusion in Fine-Grained Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6SqT8vwE6SFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confusion Matrix Generated For ConvNeXt-s Finetuned"
      ],
      "metadata": {
        "id": "_3HtAl64QCfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import models.convnext as convnext\n",
        "import os\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset/val'\n",
        "MODEL_PATH = '/content/drive/My Drive/ConvNeXt_Project/best_model_small_finetuned.pth'\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# 1. Load Model & Data\n",
        "# FIX: Initialize convnext_small to match the loaded weights\n",
        "model = convnext.convnext_small(pretrained=False)\n",
        "model.head = torch.nn.Linear(model.head.in_features, NUM_CLASSES)\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
        "model.to(DEVICE)\n",
        "model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "dataset = datasets.ImageFolder(DATA_DIR, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
        "class_names = dataset.classes\n",
        "\n",
        "# 2. Get All Predictions\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "print(\"üìä Generating Confusion Matrix...\")\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in dataloader:\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "        outputs = model(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# 3. Plot Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.title('Shortcoming: Semantic Confusion in Fine-Grained Classes')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9fPOHU8KPpDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN implementation to make comparison"
      ],
      "metadata": {
        "id": "rP04FZC5Uv44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm # <--- Ensure this is imported\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_DIR = '/content/dataset'\n",
        "SAVE_PATH = '/content/drive/My Drive/ConvNeXt_Project/best_model_simple_cnn.pth'\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "print(f\"üöÄ Starting SimpleCNN Baseline Experiment on {DEVICE}...\")\n",
        "\n",
        "# 1. DATA SETUP (Native 64x64 Resolution)\n",
        "# Note: We do NOT upscale to 224x224. This proves efficiency.\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(DATA_DIR, x), transform)\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=(x=='train'))\n",
        "               for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "\n",
        "# 2. DEFINE ARCHITECTURE\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Feature Extractor\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # 64 -> 32\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2), # 32 -> 16\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)  # 16 -> 8\n",
        "        )\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(128 * 8 * 8, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(512, NUM_CLASSES)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "model = SimpleCNN().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 3. TRAINING LOOP\n",
        "best_acc = 0.0\n",
        "print(\"\\n‚ö° Training Lightweight CNN...\")\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS}')\n",
        "\n",
        "    for phase in ['train', 'val']:\n",
        "        if phase == 'train': model.train()\n",
        "        else: model.eval()\n",
        "\n",
        "        running_corrects = 0\n",
        "\n",
        "        # --- ADDED PROGRESS BAR HERE ---\n",
        "        # desc updates dynamically to show \"Train Epoch 1\" or \"Val Epoch 1\"\n",
        "        for inputs, labels in tqdm(dataloaders[phase], desc=f\"{phase.capitalize()}\", leave=False):\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            with torch.set_grad_enabled(phase == 'train'):\n",
        "                outputs = model(inputs)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "            running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "        print(f\"   {phase.capitalize()} Acc: {epoch_acc:.4f}\")\n",
        "\n",
        "        if phase == 'val':\n",
        "            if epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                torch.save(model.state_dict(), SAVE_PATH)\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "print(f\"\\n‚úÖ Training Complete in {train_time:.1f}s\")\n",
        "print(f\"üèÜ Best Validation Accuracy: {best_acc*100:.2f}%\")\n",
        "\n",
        "# 4. ROBUSTNESS & EVALUATION PIPELINE\n",
        "print(\"\\nüß™ Running Stress Tests (Noise & Confusion Matrix)...\")\n",
        "\n",
        "# Load Best Weights\n",
        "model.load_state_dict(torch.load(SAVE_PATH, map_location=DEVICE))\n",
        "model.eval()\n",
        "\n",
        "# Metrics Containers\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "noise_levels = [0.0, 0.2, 0.4]\n",
        "noise_results = {}\n",
        "\n",
        "for noise in noise_levels:\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # Only collect detailed predictions for clean data (0.0)\n",
        "    collect_preds = (noise == 0.0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Added Progress Bar for Testing too\n",
        "        for inputs, labels in tqdm(dataloaders['val'], desc=f\"Testing Noise {noise}\", leave=False):\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # Inject Noise\n",
        "            if noise > 0:\n",
        "                noise_tensor = torch.randn_like(inputs) * noise\n",
        "                inputs = inputs + noise_tensor\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            if collect_preds:\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    noise_results[noise] = acc\n",
        "    print(f\"   Noise {noise}: {acc:.2f}%\")\n",
        "\n",
        "# 5. VISUALIZATION\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# A. Robustness Plot\n",
        "axes[0].plot(list(noise_results.keys()), list(noise_results.values()), marker='o', color='green', linewidth=2)\n",
        "axes[0].set_title(f\"SimpleCNN Robustness\\n(Training Time: {train_time:.0f}s)\", fontsize=14)\n",
        "axes[0].set_xlabel(\"Noise Level\")\n",
        "axes[0].set_ylabel(\"Accuracy (%)\")\n",
        "axes[0].grid(True)\n",
        "axes[0].set_ylim(40, 100)\n",
        "\n",
        "# B. Confusion Matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "axes[1].set_title(\"SimpleCNN Confusion Matrix (Clean)\", fontsize=14)\n",
        "axes[1].set_ylabel(\"True Label\")\n",
        "axes[1].set_xlabel(\"Predicted Label\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yOS9lEPYSILk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}